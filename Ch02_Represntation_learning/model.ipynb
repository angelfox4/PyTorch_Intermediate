{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    self.btl_size=btl_size # encoder의 output size=decoder의 input size\n",
    "    \n",
    "    super().__init__()\n",
    "    \n",
    "    self.encoder=nn.Sequential(\n",
    "        nn.Linear(28*28, 500), #MNIST 데이터셋 전용이라 생각하므로 28*28 고정\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.BatchNorm1d(500),\n",
    "        nn.Linear(500, 200),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Batchnorm1d(200),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Batchnorm1d(100),\n",
    "        nn.Linear(100, 50),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Batchnorm1d(50),\n",
    "        nn.Linear(50, 20),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Batchnorm1d(20),\n",
    "        nn.Linear(20, 10),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Batchnorm1d(10),\n",
    "        nn.Linear(10, btl_size) #output size는 btl_size, 마지막 Layer는 linear로 끝난다는 것\n",
    "    ) #btl_size는 bottleneck 현상의 그림을 보았을 때 (이 모델에서는) 10보다 크면 안좋음\n",
    "    \n",
    "    self.decoder = nn.Sequential( #nn.dencoder와 대칭적으로 짰지만, 꼭 대칭일 필요는 없음\n",
    "            nn.Linear(btl_size, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Linear(20, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.Linear(50, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Linear(200, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Linear(500, 28 * 28),#Linear layer로 끝나며, MNIST dataset에 맞추어 28*28\n",
    "        )\n",
    "        \n",
    "        def forward(self, x): #x : (batch_size, 28*28)\n",
    "            z=self.encoder(x) #z : (batch_size, btl_size=2)\n",
    "            y=self.decoder(z) #y : (batch_size, 28*28)\n",
    "            return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
