## [Summary]

**Before this chapter,** 

- objective : 우리의 세계(머릿속)에 존재하는 가상의 함수를 모사하자.
- process : 주어진 입력(x)에 대해서 원하는 출력(y)을 반환하도록, 손실함수를 최소화하는 파라미터를 찾자.
- how : Gradient descent를 수행하기 위해 back-propagation을 수행하자.

**After this chapter,**

- objective : 우리의 세계(머릿속)에 존재하는 가상의 확률 분포 함수를 모사하자.
- process : 확률 분포 P(x)에서 수집한 입력 데이터 x에 대해서 원하는 조건부 확률 분포 P(y|x) 또는 샘플링한 출력 데이터 y를 반환하도록, 손실함수를 최소화하는 확률 분포 함수의 파라미터를 찾자.
- how : 손실함수는 MLE, Information, KL-Divergence 등의 확률적 관점에 의해서 MSE, Cross Entropy error 등 선정되고, Gradient descent를 수행하기 위해 back-propagation을 수행하자.

## [File]
|File | Description|
|:-- |:-- |
|1. |Probabilistic term, Expectation, and Monte-Carlo|
|2. | MLE and Gradient Ascent|
|3. |Neural net with MLE |
|4. |DNN with MLE : Equations and Cross Entropy error |
|5. |Bayes Theorem and MAP |
|6. |DNN optimization with Kullback-Leibler Divergence|
|7. |DNN optimization with Cross Entropy|
|8. |MSE and MLE|

## [새롭게 알게 된 내용들]
**File에 pdf로 설명 요약정리**


## Reference

[김기현의 딥러닝을 활용한 자연어처리 입문 올인원 패키지 Online. | 패스트캠퍼스](https://www.fastcampus.co.kr/data_online_dpnlp)
